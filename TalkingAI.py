import os
import time
import streamlit as st
import shutil
import asyncio
import subprocess
import requests
from langchain_groq import ChatGroq
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate, # ç³»ç»Ÿ
    HumanMessagePromptTemplate # ç”¨æˆ·
)
from dotenv import load_dotenv
from deepgram import (
    DeepgramClient,
    DeepgramClientOptions,
    LiveOptions,
    LiveTranscriptionEvents,
    Microphone
)
from langchain_community.document_loaders import WebBaseLoader #æ–‡æ¡£åŠ è½½å™¨çš„ä¸€ç§ï¼Œç”¨äºåŠ è½½ç½‘é¡µå†…å®¹
from langchain.text_splitter import RecursiveCharacterTextSplitter # ç”¨äºæ–‡æ¡£åˆ†å‰²
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS # å‘é‡æ•°æ®åº“ï¼Œç”¨äºå­˜å‚¨å‘é‡æ•°æ®

from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough


load_dotenv()

class ModelProcessor:
    def __init__(self):
        self.llm = ChatGroq(
            model="llama3-70b-8192",
            api_key=os.getenv("GROQ_API_KEY"), # é»˜è®¤ä¼šä»envä¸­è·å–åä¸ºGROQ_API_KEYçš„é…ç½®ä½œä¸ºapi keyï¼Œæ‰€ä»¥å…¶å®ä¹Ÿå¯ä»¥ä¸åŠ è¿™è¡Œ
            temperature=0.1
        )
        system = SystemMessagePromptTemplate.from_template(
                """
                    Your name is Emma.
                    That is very important.
                    Your response must be under 20 words.
                """
            )
        human = HumanMessagePromptTemplate.from_template("{text}")
        self.prompt = ChatPromptTemplate.from_messages([
            system,
            human
        ])
        self.conversation = self.prompt | self.llm

    def process(self, text):
        response = self.conversation.invoke({"text": text})
        return response

class Merge_Transcript:
    def __init__(self):
        self.reset()

    def reset(self):
        self.transcript_parts = []
    
    def add_new_sentence(self, sentence):
        self.transcript_parts.append(sentence)

    def get_full_sentence(self):
        return " ".join(self.transcript_parts)

class TextToSpeech:
    Model = "aura-stella-en"

    @staticmethod # å£°æ˜é™æ€æ–¹æ³•, æ£€æŸ¥libæ˜¯å¦å®‰è£…
    def is_installed(lib_name: str):
        lib = shutil.which(lib_name)
        return lib is not None

    def speak(self, text):
        if not self.is_installed("ffplay"):
            raise ValueError("ffplay not found. if you need to use stream audio, please install it.")
        
        DEEPGRAM_URL = f"{os.environ.get('DEEPGRAM_URL')}/speak?model={self.Model}"
        headers = {
            "Authorization": f"Token {os.environ.get('DEEPGRAM_API_KEY')}",
            "Content-Type": "application/json"
        }

        payload = {"text": text}

        # åˆ›å»ºå­è¿›ç¨‹
        player_command = [
            "ffplay", 
            "-autoexit", ## æ’­æ”¾ç»“æŸè‡ªåŠ¨é€€å‡º
            "-", # æ ‡å‡†è¾“å…¥è¯»å–æ•°æ®
            "-nodisp" # ä¸è¦æ˜¾ç¤ºå›¾å½¢
        ]
        player_process = subprocess.Popen( #åˆ›å»ºä¸€ä¸ªå­è¿›ç¨‹
            player_command,
            stdin=subprocess.PIPE, # è¡¨ç¤ºåˆ›å»ºçš„å­è¿›ç¨‹çš„æ ‡å‡†è¾“å…¥ä»è¯¥è¿›ç¨‹çš„ç¨‹åºä¸­è·å–
            stdout=subprocess.DEVNULL, # ä¸éœ€è¦ï¼ŒæŠ‘åˆ¶
            stderr=subprocess.DEVNULL
        )

        with requests.post(url=DEEPGRAM_URL, headers=headers, json=payload, stream=True) as request:
            for chunk in request.iter_content(chunk_size=1024):
                if chunk:
                    player_process.stdin.write(chunk) # å°†deepgram aiä¸­è·å–åˆ°çš„éŸ³é¢‘æ•°æ®å†™å…¥å­è¿›ç¨‹ffplayä¸­æ’­æ”¾
                    player_process.stdin.flush() # å°†ç¼“å†²åŒºä¸­(flush)çš„éŸ³é¢‘æ•°æ®æ”¾åˆ°éŸ³é¢‘æ ‡å‡†è¾“å…¥æµä¸­ï¼Œè¿›è¡Œä¸‹ä¸€è½®æ’­æ”¾
                
        if player_process.stdin: # åˆ¤æ–­æ ‡å‡†è¾“å…¥æµæ˜¯å¦å­˜åœ¨
            player_process.stdin.close()
        player_process.wait() # ä¿è¯ä¸»ç¨‹åºç­‰å¾…ï¼Œç›´åˆ°æ’­æ”¾å®Œæˆ


tts = TextToSpeech()

merge_transcript = Merge_Transcript()

async def get_transcript(callback):
    transcription_complete = asyncio.Event() # ä½¿ç”¨asyncioçš„äº‹ä»¶ï¼Œå‡è½»å‹åŠ›ã€‚å½“è½¬è¯‘å®Œæˆï¼Œä¼šäº§ç”Ÿä¸€ä¸ªäº‹ä»¶

    try:
        dg_config = DeepgramClientOptions(
            options={"keepalive": "true"}
        )
        deepgram = DeepgramClient(
            os.getenv("DEEPGRAM_API_KEY"),
            dg_config
        )
        dg_connection = deepgram.listen.asynclive.v("1")
        print("Listening...")

        async def message_on(self, result,  **kwargs):
            sentence = result.channel.alternatives[0].transcript
            if not result.speech_final: # è°ˆè¯æ˜¯å¦å®Œæˆ
                merge_transcript.add_new_sentence(sentence)
            else:
                merge_transcript.add_new_sentence(sentence) # æœ€åä¸€ä¸ªå¥å­
                full_sentence = merge_transcript.get_full_sentence()
                if len(full_sentence.strip()) > 0:
                    full_sentence = full_sentence.strip()
                    print(f"Human: {full_sentence}")

                    callback(full_sentence)
                    merge_transcript.reset()

                    transcription_complete.set() # è¡¨ç¤ºè½¬è¯‘å®Œæˆ

        async def error_on(self, error, **kwargs):
            print(f"\n\n{error}\n\n")

        dg_connection.on(LiveTranscriptionEvents.Transcript, message_on)
        dg_connection.on(LiveTranscriptionEvents.Error, error_on)
        options = LiveOptions(
            model="nova-2",
            # language="zh-TW",
            language="en-US",
            encoding="linear16",
            channels=1,
            sample_rate=16000,
            smart_format=True,
            endpointing=380 #å•ä½ï¼šæ¯«ç§’ã€‚å½“åœé¡¿å¤šå°‘æ—¶é—´ï¼Œè¡¨ç¤ºä¸€æ®µç»“æŸã€‚ï¼ˆå½“è¯­éŸ³å‘é€æ—¶ï¼Œå¹¶ä¸æ˜¯ä¸€èµ·å‘çš„ï¼Œè€Œæ˜¯ä¸€æ®µä¸€æ®µå‘ï¼‰
        )
        await dg_connection.start(options)
        microphone = Microphone(dg_connection.send)
        microphone.start()

        # æ­¤å¤„åšæ— é™å¾ªç¯å¯¹å¤„ç†å™¨æœ‰å‹åŠ›ï¼
        # while True: 
        #     if not microphone.is_active():
        #         break
        #     await asyncio.sleep(5)
        await transcription_complete.wait()

        microphone.finish()
        await dg_connection.finish()
        
        print("Finished.")

    except Exception as error:
        print(f"Could not open web socket: {error}")
        return

class GetWebData:
    def __init__(self):
        self.embeddings_model = None
        self.vectorDB = None
        self.llm = ChatGroq(
            temperature=0.8,
            model="llama3-70b-8192",
            groq_api_key = os.getenv("GROQ_API_KEY")
        )

    def get_url_vectordb(self, url):
        loader = WebBaseLoader(url)
        document = loader.load()
        # st.write(document)

        # æ–‡æ¡£åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, # åˆ†å—å¤§å°
            chunk_overlap=200, # å…è®¸åˆ†å—é—´æœ‰é‡å 
            length_function=len, # ä½¿ç”¨lenæ–¹æ³•è¿›è¡Œåˆ†å‰²
            separators=["\n\n", "\n", " ", ""] # åˆ†å‰²æ ‡å¿—ç¬¦,åˆ†å‰²çš„ä½ç½®
        ) # æ–‡æ¡£åˆ†å‰²å™¨
        docs = text_splitter.split_documents(document)
        # st.write(docs)

        # å°†åˆ†å—æ•°æ®ä»æ–‡å­—è½¬æ¢ä¸ºembeddingæ ¼å¼ã€‚
        self.embeddings_model = HuggingFaceEmbeddings(
            model_name="all-MiniLM-L6-v2" # https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
        )
        # å°†åˆ†å—è½¬æ¢ä¸ºå‘é‡æ•°æ®åº“å¯ä»¥è¾¨è®¤çš„æ•°æ®
        self.vectorDB = FAISS.from_documents(docs, self.embeddings_model)
        self.vectorDB.save_local("faiss_db")
        return self.vectorDB

    def retrieval_generator(self, query, vectorDB):
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        retriever = vectorDB.as_retriever(
            search_type="similarity", # æœç´¢ç±»å‹ï¼Œç›¸ä¼¼åº¦æœç´¢
            search_kwargs={"k": 3} # æœç´¢æ—¶è¿”å›å¤šå°‘ä¸ªæœ€ç›¸ä¼¼çš„å†…å®¹
        )

        prompt = hub.pull("rlm/rag-prompt")
        rag_chain = (
            {
                "context": retriever | (lambda docs: "\n\n".join(doc.page_content for doc in docs)), # æå–æ¯ä¸€ä¸ªdocsçš„page contentï¼Œä½¿ç”¨åŒæ¢è¡Œè¿æ¥èµ·æ¥
                "question": RunnablePassthrough()
            }
            | prompt 
            | self.llm 
            | StrOutputParser()
        )
        response = rag_chain.invoke(query)
        # st.write(response)
        return response

        
        # retriever_docs = retriever.invoke(query)
        # st.write(retriever_docs)

class AiManager:
    def __init__(self):
        self.transcription_response = ""
        self.llm = ModelProcessor()  
        self.getwebdata = GetWebData()
    

    # å¼‚æ­¥å¤„ç†ï¼šè¯­éŸ³è½¬æ–‡å­—, llmå¤„ç†ï¼Œç„¶åæ–‡å­—è½¬è¯­éŸ³ï¼Œæ•´ä¸ªè¿‡ç¨‹æ¯”è¾ƒè€—æ—¶ã€‚
    async def start(self, vectorDB):
        def handle_full_sentence(full_sentence):
            self.transcription_response = full_sentence

        def stream_data(sentence, delay: float = 0.05):
            for char in sentence:
                yield char
                time.sleep(delay)

        # åˆ›å»ºä¸¤ä¸ªå ä½ç©ºé—´ï¼Œä¿è¯æé—®å’Œå›ç­”åªèƒ½åœ¨è¿™ä¸¤ä¸ªåŒºåŸŸä¸­ï¼Œä¸ä¼šå‡ºç°å†å²è®°å½•çš„æƒ…å†µ
        query_box = st.empty()
        answer_box = st.empty()

        while True:
            await get_transcript(handle_full_sentence)
            if "goodbye" in self.transcription_response.lower():
                with query_box.container(): # è‹¥æ²¡æœ‰è¿™ä¸ªçš„è¯ï¼Œå°±ä¼šå±•ç¤ºå†å²è®°å½•
                    st.write_stream(
                        stream_data(self.transcription_response)
                    )
                llm_response = self.llm.process(self.transcription_response)
                # print(llm_response.content)
                with answer_box.container():
                    st.write_stream(
                        # stream_data(llm_response.content)
                        stream_data(llm_response)
                    )
                tts.speak(llm_response.content)

                break
            
            with query_box.container(): # è‹¥æ²¡æœ‰è¿™ä¸ªçš„è¯ï¼Œå°±ä¼šå±•ç¤ºå†å²è®°å½•
                st.write_stream(
                    stream_data(self.transcription_response)
                )
            # llm_response = self.llm.process(self.transcription_response)
            llm_response = self.getwebdata.retrieval_generator(self.transcription_response, vectorDB)

            # print(llm_response.content)
            with answer_box.container():
                st.write_stream(
                    # stream_data(llm_response.content)
                    stream_data(llm_response)

                )

            tts.speak(llm_response.content)

            self.transcription_response = ""

if __name__ == "__main__":
    manager = AiManager()
    getWebData = GetWebData()

    # streamlitçš„ä¸€ä¸ªç‰¹æ€§ï¼Œå½“ä¿®æ”¹æˆ–è¾“å…¥äº†ä¸€ä¸ªå†…å®¹ï¼Œç½‘é¡µå…ƒç´ ä¼šå…¨éƒ¨åˆ·æ–°ä¸€æ¬¡ã€‚
    # ç°åœ¨å¸Œæœ›é€šè¿‡ç”¨æˆ·æäº¤ï¼Œå†è¿›è¡Œåç»­æ“ä½œã€‚
    st.title("èŠå¤©ğŸ’¬æœºå™¨äººğŸ¤–")
    st.subheader("ä½ å¯ä»¥å‘æˆ‘æé—®,æˆ‘ä¼šå°½é‡å›ç­”ä½ !")

    with st.sidebar:
        st.header("è®¾å®šï¼š")
        website_url =st.text_input(
            label="ç½‘å€",
            placeholder="è¯·è¾“å…¥æƒ³è¦æœç´¢çš„ç½‘å€"
        )
    
    if website_url is None or website_url == "":
        st.info("è¯·è¾“å…¥æ‚¨æƒ³è¯­éŸ³å›ç­”é—®é¢˜çš„ç½‘å€.")
    else:
        st.info(f"ç°åœ¨è¯­éŸ³AIå·²ç»å¯ä»¥æ ¹æ®{website_url}å›ç­”æ‚¨çš„æé—®ğŸ™‹")
        vectorDB = getWebData.get_url_vectordb(website_url)
        # getWebData.retrieval_generator("What is langchain?") # For Test
        asyncio.run(manager.start(vectorDB))